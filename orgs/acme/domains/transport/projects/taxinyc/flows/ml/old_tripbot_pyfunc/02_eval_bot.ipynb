{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f9ffe9-561d-4a1a-92a9-75e4731c49ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluate the bot\n",
    "\n",
    "Run this notebook on the serverless cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833e9a97-ded6-4ff8-a544-6b0b850ce023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with Agent Evaluation\n",
    "\n",
    "Use Mosaic AI Agent Evaluation to evaluate the agent's responses based on expected responses and other evaluation criteria. Use the evaluation criteria you specify to guide iterations, using MLflow to track the computed quality metrics.\n",
    "See Databricks documentation ([AWS]((https://docs.databricks.com/aws/generative-ai/agent-evaluation) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/)).\n",
    "\n",
    "\n",
    "To evaluate your tool calls, add custom metrics. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html#evaluating-tool-calls) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/custom-metrics#evaluating-tool-calls))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c80e7fd7-e8ba-4418-b403-c7d43df1023d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc961f56-4991-41d8-a689-7dfc9eef7fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../libs/botops/buildsetup_serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6dcfca3-89ff-418e-8f0f-b6d8bb5bbfde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Setup eval dataset\n",
    "\n",
    "This is often known as the golden data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba8a21a-61ca-489c-ae6a-7a3d1d4f3000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_examples = [\n",
    "    {\n",
    "        \"request\": {\"messages\": [{\"role\": \"user\", \"content\": \"What is an LLM agent?\"}]},\n",
    "        \"expected_response\": None,\n",
    "    },\n",
    "    {\n",
    "        \"request\": {\"messages\": [{\"role\": \"user\", \"content\": \"\"\"What is the result in python of: \n",
    "                                  \n",
    "                                  'foobar' includes 'foo'\n",
    "                                  \n",
    "                                  ?\"\"\"}]},\n",
    "        \"expected_response\": \"True\",\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_dataset = pd.DataFrame(eval_examples)\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffec8359-b6e8-4684-980c-e7e52e353373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TRIPBOT_RUN_ID = \"2acb51a1d21b45339d7f479e71063a67\"  # Replace run id with the id for the run where you logged the model\n",
    "BOT_NAME = \"tripbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af281c9-a53c-43cb-8065-500a33653ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# with mlflow.start_run(run_id=logged_agent_info.run_id):\n",
    "with mlflow.start_run(run_id=TRIPBOT_RUN_ID):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        f\"runs:/{TRIPBOT_RUN_ID}/{BOT_NAME}\",\n",
    "        data=eval_dataset,  # Your evaluation dataset\n",
    "        model_type=\"databricks-agent\",  # Enable Mosaic AI Agent Evaluation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d874f556-9c15-495e-b000-53225b2baa51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Some metrics\n",
    "eval_results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7fe0ec8-3a12-42e9-8458-be3ef6c07492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Investigate the eval results\n",
    "\n",
    "Press the `View evaluation results` button in the output above.\n",
    "\n",
    "![View evaluation results](https://github.com/paalvibe/databricks-botops-course/blob/main/images/view_evaluation_results.png)\n",
    "\n",
    "Study the Evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "579e6889-e846-496a-a5c1-4176b7a3782e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Why is there only a correctness result for the question about \"foobar includes\"? Write your reply below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b158174-f892-4dc7-99ec-b958e9bc5788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ae7dd23-bcca-462f-8f4f-1d173f5e7e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What does the Traces tab contain? Write your reply below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a583ce22-24fa-48bf-979b-abdc60398400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e7474b9-1f2b-44f5-a449-a68da671092e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What does the Artifacts tab contain? Write your reply below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d050e2a4-e045-461f-b34a-2329e0f13e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68452a19-cfbf-4323-a4c0-3e3f0ef19417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Review the evaluation results in the MLFLow UI (see console output), or access them in place:\n",
    "# display(eval_results.tables[\"eval_results\"])\n",
    "# eval_results.tables\n",
    "# Table are empty for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70eac25-2502-458a-b8d0-e43952d98aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_eval_bot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
